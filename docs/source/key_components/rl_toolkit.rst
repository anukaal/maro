RL Toolkit
==========

MARO provides a full-stack abstraction for reinforcement learning (RL) which includes various customizable
components. In order to provide a gentle introduction for the RL toolkit, we cover the components in top-down
fashion, starting from the outermost layer, the learning workflows. The RL toolkit supports single-threaded and
distributed learning workflows. The distributed workflow can be synchronous or asynchronous.


Synchronous Learning
====================

In synchronous mode, a central controler executes learning cycles that consist of simulation data collection and
policy update. In a strictly synchronous learning process, the coordinator would wait for all data collectors,
a.k.a. "roll-out workers", to return their results before moving on to the policy update phase. So what if a slow
worker drags the whole process down? We provide users the flexibility to loosen the restriction by specifying the
minimum number of workers required to receive from before proceeding to the next phase. If one is concerned about
losing precious data in the case of all workers reporting back at roughly the same time, we also provide the option
to continue to receive after receiving the minimum number of results, but with a timeout to keep the wait time
upperbounded. Note that the transition from the policy update phase to the data collection phase is still strictly
synchronous. This means that in the case of the policy instances distributed amongst a set of trainer nodes, the
central controller waits until all trainers report back with the latest policy states before starting the next
cycle.


The components required for synchronous learning include:

* Learner, which is the central coordinator for a learning process. The learner consists of a roll-out manager and
  a training manager and executes learning cycles that alternate between data collection and policy update.
* Rollout manager, which is responsible for collecting simulation data, in local or distributed fashion.
* Policy manager, which controls the policy update phase of a learning cycle. See "Policy Manager" below for details.


.. image:: ../images/rl/learner.svg
   :target: ../images/rl/learner.svg
   :alt: Overview


.. image:: ../images/rl/rollout_manager.svg
   :target: ../images/rl/rollout_manager.svg
   :alt: Overview


Asynchronous Learning
=====================

The asynchronous mode consists of a policy server and multiple actors. No central controller is present. Each data collector,
a.k.a., actor, operates independently. As soon as the server receives data from an actor, it feeds the data to the policy
manager for perform updates. The server then returns the updated policy states to the actor for the next round of data collection.
As the actors may differ significantly in speed, the policy server only uses data generated by policies sufficiently up-to-date.
but always sends the latest policy states to every single actor.

The components required for asynchronous learning include:

* actor, which alternates between sending collected simulation data to the policy server and receiving updated 
  policy states from it.
* policy server, which receives data from the actors and update the policy states when necessary.


Environment Wrapper
-------------------

To use the training components described above, it is necessary to implement an environment wrapper for the environment of
your choice. An environment wrapper serves as a bridge between a simulator and the policies that interact with it by providing
unified interfaces to the interaction workflow. It is also responsible for caching transitions and preparing experiences for
training. Key methods that need to be implemented for an environment wrapper include:

* ``get_state``, which encodes agents' observations into policy input. The encoded state for each agent must correspond
    to the policy used by the agent.
* ``to_env_action``, which provides model output with context so that it can be executed by the environment simulator.
* ``get_reward``, for evaluating rewards.

.. image:: ../images/rl/env_wrapper.svg
   :target: ../images/rl/env_wrapper.svg
   :alt: Environment Wrapper


Policy
------

A policy is a an agent's mechanism to choose actions based on its observations of the environment.
Accordingly, the abstract ``AbsPolicy`` class exposes a ``choose_action`` interface. This abstraction encompasses
both static policies, such as rule-based policies, and updatable policies, such as RL policies. The latter is
abstracted through the ``AbsCorePolicy`` sub-class which also exposes a ``update`` interface. By default, updatable
policies require an experience manager to store and retrieve simulation data (in the form of "experiences sets")
based on which updates can be made.


.. code-block:: python

  class AbsPolicy(ABC):
      @abstractmethod
      def choose_action(self, state):
          raise NotImplementedError


  class AbsCorePolicy(AbsPolicy):
      def __init__(self, experience_store: ExperienceStore):
          super().__init__()
          self.experience_memory = experience_store

      @abstractmethod
      def update(self):
          raise NotImplementedError


Policy Manager
--------------

A policy manager is an abstraction that controls policy update. It houses the latest policy states.
In synchrounous learning, the policy manager controls the policy update phase of a learning cycle.
In asynchrounous learning, the policy manager is the centerpiece of the policy server process. 
Individual policy updates, however, may or may not occur within the policy manager itself depending
on the policy manager type used. The provided policy manager classes include:

* ``LocalPolicyManager``, where the policies are updated within the manager itself;
* ``MultiProcessPolicyManager``, which distributes policies amongst a set of trainer processes to parallelize
  policy update;
* ``MultiNodePolicyManager``, which distributes policies amongst a set of remote compute nodes to parallelize
  policy update, but for each policy, only one node would be assigned to. Thus the node number should be less
  than or equal to policy number;
* ``MultiNodeDistPolicyManager``, which distributes policies amongst a set of remote compute nodes to parallelize
  policy update, while supporting to divide one policy to multiple remote compute nodes. The node number can
  be greater than the policy number. Moreover, it will perform auto-balance during training, which dynamically
  adjusts the number of compute nodes for policies according to their experience number.

.. image:: ../images/rl/policy_manager.svg
    :target: ../images/rl/policy_manager.svg
    :alt: RL Overview

Core Model
----------

In the deep reinforcement learning (DRL) world, a core policy usually includes one or more neural-network-based models,
which may be used to compute action preferences or estimate state / action values. The core model abstraction is designed
to decouple the the inner workings of these models from the algorithmic aspects of the policy that uses them. For example,
the actor-critic algorithm does not need to concern itself with the structures and optimizing schemes of the actor and
critic models. The ``AbsCoreModel`` abstraction represents a collection of network components with embedded optimizers.
Subclasses of ``AbsCoreModel`` provided for use with specific RL algorithms include ``DiscreteQNet`` for DQN, ``DiscretePolicyNet``
for Policy Gradient, ``DiscreteACNet`` for Actor-Critic and ``ContinuousACNet`` for DDPG.

The code snippet below shows how to create a model for the actor-critic algorithm with a shared bottom stack:

.. code-block:: python

  class MyACModel(DiscreteACNet):
      def forward(self, states, actor=True, critic=True):
          features = self.component["representation"](states)
          return (
              self.component["actor"](features) if actor else None,
              self.component["critic"](features) if critic else None
          )


  representation_stack = FullyConnectedBlock(...)
  actor_head = FullyConnectedBlock(...)
  critic_head = FullyConnectedBlock(...)
  ac_model = SimpleMultiHeadModel(
      {"representation": representation_stack, "actor": actor_head, "critic": critic_head},
      optim_option={
        "representation": OptimizerOption(cls="adam", params={"lr": 0.0001}),
        "actor": OptimizerOption(cls="adam", params={"lr": 0.001}),
        "critic": OptimizerOption(cls="rmsprop", params={"lr": 0.0001})  
      }
  )

To generate stochastic actions given a batch of states, call ``get_action`` on the model instance: 

.. code-block:: python

  action, log_p = ac_model.get_action(state)

To performing a single gradient step on the model, call the ``step`` function: 

.. code-block:: python

  ac_model.step(critic_loss + actor_loss)

Here it is assumed that the losses have been computed using the same model instance and the gradients have
been generated for the internal components.  


Experience
----------

An ``ExperienceSet`` is a synonym for training data for RL policies. The data originate from the simulator and
get processed and organized into a set of transitions in the form of (state, action, reward, next_state, info),
where ''info'' contains information about the transition that is not encoded in the state but may be necessary
for sampling purposes. An ``ExperienceStore`` is a storage facility for experience sets and is maintained by
a policy for storing and retrieving training data. Sampling from the experience memory can be customized by 
registering a user-defined sampler to it.  

``ExperienceSet`` offers a list-like usage:

.. code-block:: python

    experience_set = ExperienceSet(states, actions, rewards, next_states, info)
    # length
    print(len(experience_set))
    # or
    print(experience_set.size)
    # index
    experience_batch = experience_set[0]
    # slice
    experience_batch = experience_set[0:5]
    # slice with stride
    experience_batch = experience_set[0:20:5]
    # extend
    experience_set.extend(experience_batch)

Exploration
-----------

Some RL algorithms such as DQN and DDPG require explicit exploration governed by a set of parameters. The
``AbsExploration`` class is designed to cater to these needs. Simple exploration schemes, such as ``EpsilonGreedyExploration`` for discrete action space
and ``UniformNoiseExploration`` and ``GaussianNoiseExploration`` for continuous action space, are provided in
the toolkit.

As an example, the exploration for DQN may be carried out with the aid of an ``EpsilonGreedyExploration``:

.. code-block:: python

  exploration = EpsilonGreedyExploration(num_actions=10)
  greedy_action = q_net.get_action(state)
  exploration_action = exploration(greedy_action)
