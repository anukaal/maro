# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import time
from os import getcwd
from typing import Callable, List, Union

from maro.utils import Logger

from .early_stopper import AbsEarlyStopper
from .env_sampler import AbsEnvSampler
from .helpers import get_eval_schedule, get_rollout_finish_msg
from .policy_manager import AbsPolicyManager
from .rollout_manager import AbsRolloutManager


def simple_learner(
    get_env_sampler: Callable[[], AbsEnvSampler],
    num_episodes: int,
    num_steps: int = -1,
    eval_schedule: Union[int, List[int]] = None,
    eval_after_last_episode: bool = True,
    early_stopper: AbsEarlyStopper = None,
    post_collect: Callable = None,
    post_evaluate: Callable = None,
    log_dir: str = getcwd()
):
    """Single-threaded learning workflow.

    Args:
        get_env_sampler (Callable): Function to create an environment wrapper for collecting training data. The function
            should take no parameters and return an environment wrapper instance.
        get_agent_wrapper (Callable): Function to create an agent wrapper that interacts with the environment wrapper.
            The function should take no parameters and return a ``AgentWrapper`` instance.
        num_episodes (int): Number of training episodes. Each training episode may contain one or more
            collect-update cycles, depending on how the implementation of the roll-out manager.
        num_steps (int): Number of environment steps to roll out in each call to ``collect``. Defaults to -1, in which
            case the roll-out will be executed until the end of the environment.
        get_test_env_wrapper (Callable): Function to create an environment wrapper for evaluation. The function should
            take no parameters and return an environment wrapper instance. If this is None, the training environment
            wrapper will be used for evaluation in the worker processes. Defaults to None.
        eval_schedule (Union[int, List[int]]): Evaluation schedule. If an integer is provided, the policies will
            will be evaluated every ``eval_schedule`` episodes. If a list is provided, the policies will be evaluated
            at the end of the episodes given in the list. Defaults to None, in which case no evaluation is performed
            unless ``eval_after_last_episode`` is set to True.
        eval_after_last_episode (bool): If True, the policy will be evaluated after the last episode of learning is
            finished. Defaults to True.
        early_stopper (AbsEarlyStopper): Early stopper to stop the main training loop if certain conditions on the
            environment metrics are met following an evaluation episode. Default to None.
        post_collect (Callable): Custom function to process whatever information is collected by each
            environment wrapper (local or remote) at the end of ``collect`` calls. The function signature should
            be (trackers,) -> None, where tracker is a list of environment wrappers' ``tracker`` members. Defaults
            to None.
        post_evaluate (Callable): Custom function to process whatever information is collected by each
            environment wrapper (local or remote) at the end of ``evaluate`` calls. The function signature should
            be (trackers,) -> None, where tracker is a list of environment wrappers' ``tracker`` members. Defaults
            to None.
        log_dir (str): Directory to store logs in. A ``Logger`` with tag "LOCAL_ROLLOUT_MANAGER" will be created at init
            time and this directory will be used to save the log files generated by it. Defaults to the current working
            directory.
    """
    if num_steps == 0 or num_steps < -1:
        raise ValueError("num_steps must be a positive integer or -1")

    logger = Logger("LOCAL_LEARNER", dump_folder=log_dir)
    env_sampler = get_env_sampler()

    # evaluation schedule
    eval_schedule = get_eval_schedule(eval_schedule, num_episodes, final=eval_after_last_episode)
    logger.info(f"Policy will be evaluated at the end of episodes {eval_schedule}")
    eval_point_index = 0

    def collect_and_update(ep, exploration_step):
        """Collect simulation data for training."""
        segment = 1
        while True:
            result = env_sampler.sample(
                num_steps=num_steps, exploration_step=exploration_step, return_rollout_info=False
            )
            logger.info(
                get_rollout_finish_msg(ep, result["step_range"], exploration_params=result["exploration_params"])
            )
            env_sampler.agent_wrapper.improve()

            if post_collect:
                post_collect([result["tracker"]], ep, segment)

            if result["end_of_episode"]:
                break

            segment += 1

    exploration_step = False
    for ep in range(1, num_episodes + 1):
        collect_and_update(ep, exploration_step)
        exploration_step = True
        if ep == eval_schedule[eval_point_index]:
            eval_point_index += 1
            tracker = env_sampler.test()
            if post_evaluate:
                post_evaluate([tracker], eval_point_index)
            # early stopping check
            if early_stopper:
                early_stopper.push(env_sampler.test_env.summary)
                if early_stopper.stop():
                    return


class Learner:
    """Main controller for learning.

    This should be used in multi-process or distributed settings where either the policy manager or the roll-out
    manager has a distributed architecture. For pure local learning workflows, using this may cause pitfalls such
    as duplicate experience storage. Use ``SimpleLearner`` instead.

    Args:
        policy_manager (AbsPolicyManager): An ``AbsPolicyManager`` instance that controls policy updates.
        rollout_manager (AbsRolloutManager): An ``AbsRolloutManager`` instance that controls simulation data
            collection.
        num_episodes (int): Number of training episodes. Each training episode may contain one or more
            collect-update cycles, depending on the implementation of the roll-out manager.
        eval_schedule (Union[int, List[int]]): Evaluation schedule. If an integer is provided, the policies will
            will be evaluated every ``eval_schedule`` episodes. If a list is provided, the policies will be evaluated
            at the end of the episodes given in the list. Defaults to None, in which case no evaluation is performed
            unless ``eval_after_last_episode`` is set to True.
        eval_after_last_episode (bool): If True, the policy will be evaluated after the last episode of learning is
            finished. Defaults to False.
        early_stopper (AbsEarlyStopper): Early stopper to stop the main training loop if certain conditions on the
            environment metric are met following an evaluation episode. Default to None.
        log_dir (str): Directory to store logs in. A ``Logger`` with tag "LEARNER" will be created at init time
            and this directory will be used to save the log files generated by it. Defaults to the current working
            directory.
    """
    def __init__(
        self,
        policy_manager: AbsPolicyManager,
        rollout_manager: AbsRolloutManager,
        num_episodes: int,
        eval_schedule: Union[int, List[int]] = None,
        eval_after_last_episode: bool = False,
        early_stopper: AbsEarlyStopper = None,
        log_dir: str = getcwd()
    ):
        self._logger = Logger("LEARNER", dump_folder=log_dir)
        self.policy_manager = policy_manager
        self.rollout_manager = rollout_manager

        self.num_episodes = num_episodes

        # evaluation schedule
        self._eval_schedule = get_eval_schedule(eval_schedule, num_episodes, final=eval_after_last_episode)
        self._logger.info(f"Policy will be evaluated at the end of episodes {self._eval_schedule}")
        self._eval_point_index = 0

        self.early_stopper = early_stopper

    def run(self):
        """Entry point for executing a learning workflow."""
        for ep in range(1, self.num_episodes + 1):
            self._collect_and_update(ep)
            if ep == self._eval_schedule[self._eval_point_index]:
                self._eval_point_index += 1
                env_metric_dict = self.rollout_manager.evaluate(ep, self.policy_manager.get_state())
                # early stopping check
                if self.early_stopper:
                    for env_metric in env_metric_dict.values():
                        self.early_stopper.push(env_metric)
                        if self.early_stopper.stop():
                            return

        if hasattr(self.rollout_manager, "exit"):
            self.rollout_manager.exit()

        if hasattr(self.policy_manager, "exit"):
            self.policy_manager.exit()

    def _collect_and_update(self, ep: int):
        collect_time = policy_update_time = 0
        self.rollout_manager.reset()
        segment = 1
        while not self.rollout_manager.episode_complete:
            # experience collection
            policy_state_dict, policy_version = self.policy_manager.get_state(), self.policy_manager.get_version()
            tc0 = time.time()
            rollout_info_by_policy = self.rollout_manager.collect(ep, segment, policy_state_dict, policy_version)
            collect_time += time.time() - tc0
            tu0 = time.time()
            self.policy_manager.update(rollout_info_by_policy)
            policy_update_time += time.time() - tu0
            segment += 1

        # performance details
        self._logger.info(
            f"ep {ep} summary - "
            f"collect time: {collect_time} "
            f"policy update time: {policy_update_time}"
        )
